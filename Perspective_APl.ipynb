{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set up a Perspective API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'AIzaSyDC57I66ebBlhcXkeovtnL_aitk75b6cDA'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore the sample dataset to form hypotheses\n",
    "We will load the dataset into a Pandas DataFrame, print the number of comments and the distribution of toxicity labels in the dataset, get the subset of comments that are toxic, split the toxic comments into individual words, and print the most common words used in toxic comments. It will also print a sample of 10 toxic comments for visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments: 55252 \n",
      "\n",
      "Toxicity label distribution:\n",
      "no     51409\n",
      "yes     3843\n",
      "Name: toxic, dtype: int64 \n",
      "\n",
      "Number of toxic comments: 3843 \n",
      "\n",
      "Most common words used in toxic comments:\n",
      "a       2774\n",
      "the     2450\n",
      "is      2237\n",
      "and     2057\n",
      "to      2053\n",
      "you     2011\n",
      "I       1758\n",
      "of      1463\n",
      "that    1014\n",
      "boob    1001\n",
      "dtype: int64 \n",
      "\n",
      "Visual inspection of the comments:\n",
      "1 :   Tanner Bobbins   WHY WOULD YOU DELETE HIM THERE IS NOTHING HE CAN LOOK FORWARD TO NOW HE WILL THINK HE IS A LOSER BECAUSE NO ONE KNOWS HIM I WILL KEEP MAKING NEW PAGES AND NEW PAGES UNTIL YOU DECIDE TO STOP DELETING THEM BOBBINS IS TOTALLY WORTH BEING ON WIKIPEDIA AND YOU KNOW IT IF HE KILLS HIMSELF THE BLOOD WILL BE ON YOUR HANDS AND YOUR CHILDRENS FOREVER WE WILL NEVER STOP MAKING THESE PAGES\n",
      "2 :  HAVE A WET VAGINA\n",
      "3 :    Because stupid me I thought I try and do the right thing even though time was short More fool me eh  \n",
      "4 :  UTC     Newsflash   No I am afraid not America cant be made stupid  its already stupid Yet one more reason why Chinese dont care about Google  it is run by idiots   0344 3 March 2010\n",
      "5 :  blog is gay and pointless\n",
      "6 :  Are you retarded aild are metalcore waking the cadaver is goregrind\n",
      "7 :  YOU ARE THE WORST YOU HAVE NO INTEGRITY\n",
      "8 :  fuke you nigas  asfgh jihgijfhgjfhgfkghfdgvhifgvhd bakhg hjhgdjhfgfgkjhgf hdghd jhgjgfj hdfgf jvhjgvhdvfg\n",
      "9 :  Thats just stupid how can i be blocked for stating a fact The source is there all you have to do is click on the website and you have updated sales Why would i inflate the sales if i were to do that i would do more than 14000 more Anyway its sorted now P\n",
      "10 :  aleesha deion are u gay\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "    \n",
    "# Load the dataset into a Pandas DataFrame\n",
    "df = pd.read_csv(\"Sample_labaled_data.csv\")\n",
    "\n",
    "# Print the number of comments in the dataset\n",
    "print(\"Number of comments:\", len(df), '\\n')\n",
    "\n",
    "# Print the distribution of toxicity labels in the dataset\n",
    "print(\"Toxicity label distribution:\")\n",
    "print(df[\"toxic\"].value_counts(), '\\n')\n",
    "\n",
    "# Get the subset of comments that are toxic\n",
    "toxic_comments = df[df[\"toxic\"] == \"yes\"]\n",
    "\n",
    "# Print the number of toxic comments\n",
    "print(\"Number of toxic comments:\", len(toxic_comments), '\\n')\n",
    "\n",
    "# Split the toxic comments into individual words\n",
    "toxic_words = []\n",
    "for comment in toxic_comments[\"comment_text\"]:\n",
    "    words = comment.split()\n",
    "    toxic_words.extend(words)\n",
    "\n",
    "\n",
    "# Print the most common words used in toxic comments\n",
    "print(\"Most common words used in toxic comments:\")\n",
    "word_counts = pd.Series(toxic_words).value_counts()\n",
    "print(word_counts.head(10), '\\n')\n",
    "\n",
    "# Visual inspection of the comments can also reveal potential biases\n",
    "print(\"Visual inspection of the comments:\")\n",
    "for i, comment in enumerate(toxic_comments[\"comment_text\"].sample(10)):\n",
    "    print(i + 1, \": \", comment)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Form hypotheses,Design andperform tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from googleapiclient.errors import HttpError \n",
    "from langdetect import detect\n",
    "import time \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the csv file, there are languages besides English, so we need to filter out languages that are not English. Moreover, there are limits on the number of API requests we can make per minute, so we need to wait until the rate limit is reset (usually within a minute). However, the csv file contains more than 50,000 comments which may take 10 hours to process. Thus, we randomly chose 500 hundred comments as our dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55252/55252 [06:25<00:00, 143.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define a function to detect the language of a comment\n",
    "def detect_language(comment):\n",
    "    try:\n",
    "        if detect(comment) == \"en\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "tqdm.pandas()\n",
    "# Apply the language detection function to filter the dataframe for English comments\n",
    "df = df[df[\"comment_text\"].progress_apply(detect_language)]\n",
    "\n",
    "# Select a random sample of 500 comments\n",
    "df = df.sample(500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then send a request to the Perspective API to get the toxicity scores for the comments. It will then determine the threshold for the model based on the 95th percentile of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 197/500 [04:00<01:54,  2.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An HTTP error 429 occurred:\n",
      "Quota exceeded for quota metric 'Analysis requests (AnalyzeComment)' and limit 'Analysis requests (AnalyzeComment) per minute' of service 'commentanalyzer.googleapis.com' for consumer 'project_number:910082442830'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 238/500 [04:13<01:26,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An HTTP error 400 occurred:\n",
      "Attribute TOXICITY does not support request languages: ilo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 307/500 [06:35<05:57,  1.85s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An HTTP error 429 occurred:\n",
      "Quota exceeded for quota metric 'Analysis requests (AnalyzeComment)' and limit 'Analysis requests (AnalyzeComment) per minute' of service 'commentanalyzer.googleapis.com' for consumer 'project_number:910082442830'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 327/500 [06:41<00:48,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An HTTP error 400 occurred:\n",
      "Attribute TOXICITY does not support request languages: om\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [10:35<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.73174625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Send a request to the Perspective API to get the toxicity scores for the training set\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "train_scores = []\n",
    "i = 0\n",
    "for comment in tqdm((df[\"comment_text\"])):\n",
    "    i += 1\n",
    "    data = {\n",
    "        \"comment\": {\"text\": comment},\n",
    "        \"requestedAttributes\": {\"TOXICITY\": {}}\n",
    "    }\n",
    "    try:\n",
    "        response = client.comments().analyze(body=data).execute()\n",
    "        score = response[\"attributeScores\"][\"TOXICITY\"][\"summaryScore\"][\"value\"]\n",
    "        train_scores.append(score)\n",
    "        if i % 60 == 0:\n",
    "            time.sleep(60)\n",
    "    except HttpError as error:\n",
    "        print(f\"An HTTP error {error.resp.status} occurred:\\n{error._get_reason()}\")\n",
    "\n",
    "# Determine the threshold for the model based on the scores\n",
    "threshold = pd.Series(train_scores).quantile(0.95)\n",
    "print(\"Threshold:\", threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will finally test the hypothesis that Perspective will be less likely to mark comments `without dirty words` as toxic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity scores for test set:\n",
      "I love my cat : 0.027324399\n",
      "The weather is beautiful today : 0.014890783\n",
      "I had a great time at the park : 0.022964042\n",
      "The specific page linked to is just junk in regards to our policies on sources  check it for yourself : 0.4575469\n",
      "If the indigenous population is so low why do they think they deserve everything : 0.31547862\n",
      "Two short stories  The list of works contains something called The Mysterious Portrait Is this the same as the short story The Portrait : 0.02048268\n",
      "I had changed the SS Template Its all finished and all right now : 0.024849601\n",
      "I said is going to be a longgggg one : 0.054531995\n",
      "Sounds good to me : 0.021903414\n",
      "Basic chemistry fails to describe whats going on hence the need for Biochemistry : 0.027324399\n",
      "Perspective is less likely to mark comments without dirty words as toxic.\n"
     ]
    }
   ],
   "source": [
    "# Create a test set of comments without dirty words\n",
    "test_comments = [\n",
    "    \"I love my cat\",\n",
    "    \"The weather is beautiful today\",\n",
    "    \"I had a great time at the park\",\n",
    "    \"The specific page linked to is just junk in regards to our policies on sources  check it for yourself\",\n",
    "    \"If the indigenous population is so low why do they think they deserve everything\",\n",
    "    \"Two short stories  The list of works contains something called The Mysterious Portrait Is this the same as the short story The Portrait\",\n",
    "    \"I had changed the SS Template Its all finished and all right now\",\n",
    "    \"I said is going to be a longgggg one\",\n",
    "    \"Sounds good to me\",\n",
    "    \"Basic chemistry fails to describe whats going on hence the need for Biochemistry\"\n",
    "]\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "# Send a request to the Perspective API to get the toxicity scores for the test set\n",
    "test_scores = []\n",
    "print(\"Toxicity scores for test set:\")\n",
    "for comment in test_comments:\n",
    "    data = {\n",
    "        \"comment\": {\"text\": comment},\n",
    "        \"requestedAttributes\": {\"TOXICITY\": {}},\n",
    "        \"languages\": [\"en\"]\n",
    "    }\n",
    "    response = client.comments().analyze(body=data).execute()\n",
    "    try:\n",
    "        response = client.comments().analyze(body=data).execute()\n",
    "        score = response[\"attributeScores\"][\"TOXICITY\"][\"summaryScore\"][\"value\"]\n",
    "        test_scores.append(score)\n",
    "        print(comment, \":\", score)\n",
    "    except HttpError as error:\n",
    "        print(f\"An HTTP error {error.resp.status} occurred:\\n{error._get_reason()}\")\n",
    "\n",
    "# Compare the scores to the hypothesis\n",
    "if all(score < threshold for score in test_scores):\n",
    "    print(\"Perspective is less likely to mark comments without dirty words as toxic.\")\n",
    "else:\n",
    "    print(\"Perspective does not appear to be less likely to mark comments without dirty words as toxic.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Write about your results\n",
    "\n",
    "Based on the testing we performed, it appears that the Perspective API is less likely to mark comments without dirty words as toxic. This result may not come as a surprise, as the API was designed to identify toxic language and dirty words are often associated with such language. However, it is important to note that this result only applies to the specific test set we used and may not generalize to other datasets or contexts.  \n",
    "\n",
    "It is possible that biases exist in the model based on the data it was trained on and the criteria used to label comments as toxic. For example, the training data may be biased towards certain demographics or types of language, which could influence the model's ability to accurately identify toxicity in other contexts. Additionally, the criteria used to label comments as toxic may be based on subjective judgments and may not be applicable or relevant in all contexts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
